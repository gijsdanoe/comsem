I/ Baseline_2:

1. skip token:

- check if token is pronoun: yes -> skip (POS tag)
- check if digit: yes -> skip
- check if token is prop name: yes -> skip (POS tag)
- check if token is punctuation: yes -> skip


2. create the temp_lookupdict from trainset
temp_dict = {}


both no:
	check if token in temp_dict:
		no: add it and its synset/ "O" (don't need to check if searchable in WN here because in the data you already know which one is "O")
		yes:
			check if its synset in temp_dict[token]:
				if no: add its synset
				if yes: count +1
		- collect all the synsets appear in trainset and save in temp_dict

		return temp_dict

3. create lookup dict from trainset
loopkup_dict = {}
for each token in temp_dict:
	- append token in dict
	- select the most frequent synset and add it in loopkup_dict[token]

return loopkup_dict

4. baseline system
- input an raw sentence
- tokenize the sentence
- for each token in sentence:
	- check skip token
	- check if the token in lookup_dict:
		yes: take its synset
		no:
			- check if token is searchable in WN:
				+ yes: get the 1st synset match pos
				+ if no synset match pos, get the 1st synset.
				+ no: get "O"

- return {tokens, synsets}

5. Test bl on testset, devset, evalset:
- input sentences of each set
- use bl to get synset of the target tokens of these sentence
- output is saved in a csv file with tokens and labels

6. Create gold labels files from each set to evaluate bl:
- use tokens and labels of each files
- these tokens are filtered as the same way of bl to get the consistent tokens with the tokens in the output files.

7. Evaluation (on each set):
- save gold tokens labels (created from 6), and labels produced by bl into a csv file (easy for comparision and error analysis)
- if both labels are same, get "1", otherwise "0"
- aslo check the Leacock Chodorow Similarity between the gold and predict labels to see if they are the same meaning. If yes, get "1", otherwise "0".
- count the 1's and divide the count to total of tokens/labels.

