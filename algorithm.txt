1. create the temp_lookupdict from trainset
temp_dict = {}

- check if token is pronoun: yes -> skip
- check if token is prop name: yes -> skip
both no:
	check if token in temp_dict:
		no: add it and its synset/ "O" (don't need to check if searchable in WN here because in the data you already know which one is "O")
		yes:
			check if its synset in temp_dict[token]:
				if no: add its synset
				if yes: count +1
		- collect all the synsets appear in trainset and save in temp_dict

		return temp_dict

2. create lookup dict from trainset
dict = {}
for each token in temp_dict:
	- append token in dict
	- select the most frequent synset and add it in dict[token]

return loopkup_dict

3. baseline system
- input an raw sentence
- tokenize the sentence
- dict = {token: synset}
- for each token in sentence:
	- check if the token in lookup_dict:
		yes: take its synset, add to dict[token]
		no:
			- check if token is searchable in WN:
				+ no: get "O", add to dict[token]
				+ yes: get the 1st synset, add to dict[token]

- return dict

4. main
create a csv with 1 column token, 1 column data label, 1 column baseline label, 1 column 1 and 0's
